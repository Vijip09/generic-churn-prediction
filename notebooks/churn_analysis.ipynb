# Importing required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, classification_report

# Load the dataset
data = pd.read_csv('telecom_churn.csv')

# Quick glimpse of the dataset
print("Dataset Shape:", data.shape)
print(data.head())

# 1. Data Preprocessing & EDA

# Checking for missing values
print("\nMissing Values:\n", data.isnull().sum())

# Replacing any missing values (if any) with the median
data.fillna(data.median(), inplace=True)

# Summary statistics of the dataset
print("\nDataset Summary:\n", data.describe())

# Visualizing the distribution of the target variable 'churn'
plt.figure(figsize=(6,4))
sns.countplot(x='churn', data=data)
plt.title('Distribution of Churn')
plt.show()

# Checking correlation between features using a heatmap
plt.figure(figsize=(12,8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title('Feature Correlation Heatmap')
plt.show()

# 2. Feature Engineering
# ----------------------------------

# Dropping irrelevant features (if any)
# For instance, we could drop features with too much correlation or those that are not useful
# Here we are keeping all features for simplicity, but adjust as necessary

# Creating the feature matrix X and the target vector y
X = data.drop(['churn'], axis=1)
y = data['churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features (to bring them to the same scale)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 3. Model Selection & Training
# ----------------------------------

# Initializing models
logreg = LogisticRegression(random_state=42)
rf = RandomForestClassifier(random_state=42)
xgb = XGBClassifier(random_state=42)

# Training Logistic Regression model
logreg.fit(X_train, y_train)

# Training Random Forest model
rf.fit(X_train, y_train)

# Training XGBoost model
xgb.fit(X_train, y_train)

# 4. Model Evaluation
# ----------------------------------

# Function to evaluate models
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    print(f"\nModel: {model.__class__.__name__}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"AUC-ROC Score: {auc:.4f}")
    print("\nConfusion Matrix:\n", cm)
    print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Evaluating all models
evaluate_model(logreg, X_test, y_test)
evaluate_model(rf, X_test, y_test)
evaluate_model(xgb, X_test, y_test)

